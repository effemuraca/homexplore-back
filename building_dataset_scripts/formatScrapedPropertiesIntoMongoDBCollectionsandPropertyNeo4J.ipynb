{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjWUI_h4DeBK","executionInfo":{"status":"ok","timestamp":1739373954229,"user_tz":-60,"elapsed":18028,"user":{"displayName":"CARLO VINCENZO STANZIONE","userId":"04219436989035910339"}},"outputId":"683ede95-bb58-447b-a35a-4e8d4db7e51b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n"]}]},{"cell_type":"code","source":["!pip install faker\n","!pip install bcrypt\n","!pip install bson"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYVrFElaJJBv","executionInfo":{"status":"ok","timestamp":1739373968475,"user_tz":-60,"elapsed":14254,"user":{"displayName":"CARLO VINCENZO STANZIONE","userId":"04219436989035910339"}},"outputId":"8f41c264-a1c7-49e5-c649-ce1fc8b0ba9d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faker\n","  Downloading Faker-36.1.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n","Downloading Faker-36.1.0-py3-none-any.whl (1.9 MB)\n","\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.1/1.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.7/1.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faker\n","Successfully installed faker-36.1.0\n","Collecting bcrypt\n","  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n","Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bcrypt\n","Successfully installed bcrypt-4.2.1\n","Collecting bson\n","  Downloading bson-0.5.10.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from bson) (2.8.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from bson) (1.17.0)\n","Building wheels for collected packages: bson\n","  Building wheel for bson (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bson: filename=bson-0.5.10-py3-none-any.whl size=11975 sha256=9edb26be5fe8a51926c96cb5ece4810f7a5264198e4cbf830232f1f25a5b56d1\n","  Stored in directory: /root/.cache/pip/wheels/cb/f3/45/c859e83339943dfe2f43e1c9aaebdc00db321191a6fe120947\n","Successfully built bson\n","Installing collected packages: bson\n","Successfully installed bson-0.5.10\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8cPRm9eCF5I","outputId":"e8c82b69-40a7-4e76-dd17-c165556b8354","executionInfo":{"status":"ok","timestamp":1739374047488,"user_tz":-60,"elapsed":79019,"user":{"displayName":"CARLO VINCENZO STANZIONE","userId":"04219436989035910339"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['properties_new york_ny.csv', 'properties_los angeles_ca.csv', 'properties_chicago_il.csv', 'properties_houston_tx.csv', 'properties_phoenix_az.csv', 'properties_philadelphia_pa.csv', 'properties_san antonio_tx.csv', 'properties_san diego_ca.csv', 'properties_dallas_tx.csv', 'properties_austin_tx.csv', 'properties_jacksonville_fl.csv', 'properties_san jose_ca.csv', 'properties_indianapolis_in.csv', 'properties_san francisco_ca.csv', 'properties_columbus_oh.csv', 'properties_fort worth_tx.csv', 'properties_charlotte_nc.csv', 'properties_oklahoma city_ok.csv', 'properties_detroit_mi.csv']\n","['sellers.csv', 'properties_on_sale_neo4j.csv', 'properties_on_sale.csv', 'buyers.csv', 'reservations_seller.csv', 'reservations_buyer.csv']\n"]}],"source":["import os\n","import pandas as pd\n","import random\n","from math import floor, radians, sin, cos, sqrt, atan2\n","import ast\n","import json\n","from faker import Faker\n","from bcrypt import hashpw, gensalt\n","from bson import ObjectId\n","from datetime import datetime, timedelta\n","\n","fake = Faker()\n","\n","# Function to hash passwords\n","def hash_password(password: str) -> str:\n","    return hashpw(password.encode('utf-8'), gensalt(rounds=4)).decode('utf-8')\n","\n","# Function to calculate the distance between two geographic points (latitude, longitude)\n","def haversine(lat1, lon1, lat2, lon2):\n","    R = 6371\n","    dlat = radians(lat2 - lat1)\n","    dlon = radians(lon2 - lon1)\n","    a = sin(dlat / 2) ** 2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2) ** 2\n","    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n","    return R * c\n","\n","# Function to get the nearest neighbourhood\n","def get_neighbourhood(lat, lon, city):\n","    city = city.title()\n","    city_neighbourhoods = neighbourhoods_df[neighbourhoods_df['city'] == city].copy()\n","    if city_neighbourhoods.empty:\n","        print(f\"No neighbourhood found for city: {city}\")\n","        return ''\n","    city_neighbourhoods['distance'] = city_neighbourhoods.apply(\n","        lambda row: haversine(lat, lon, row['latitude'], row['longitude']), axis=1\n","    )\n","    nearest_neighbourhood = city_neighbourhoods.loc[city_neighbourhoods['distance'].idxmin()]\n","    return nearest_neighbourhood['name']\n","\n","# Function to generate buyers\n","def generate_buyers(properties_on_sale_flat, output_folder):\n","    buyers = []\n","    # Group unsold properties by city\n","    city_properties = {}\n","    for prop in properties_on_sale_flat:\n","        summary = {\n","            '_id': prop['_id'],\n","            'thumbnail': prop['thumbnail'],\n","            'address': prop['address'],\n","            'price': prop['price'],\n","            'area': prop['area']\n","        }\n","        city = prop['city']\n","        if city not in city_properties:\n","            city_properties[city] = []\n","        city_properties[city].append(summary)\n","\n","    # For each city, generate a random number of buyers (between 100 and 300)\n","    for city, props in city_properties.items():\n","        num_buyers = random.randint(100, 300)\n","        for _ in range(num_buyers):\n","            fake_first_name = fake.first_name()\n","            fake_last_name = fake.last_name()\n","            buyer = {\n","                '_id': str(ObjectId()),\n","                'name': fake_first_name,\n","                'surname': fake_last_name,\n","                'email': fake_first_name.lower() + fake_last_name.lower() + '@buyer.com',\n","                'password': hash_password(fake.password()),\n","                'phone_number': fake.phone_number(),\n","                'favourites': []\n","            }\n","            # Randomly select between 1 and 3 properties as favourites from the same city\n","            num_favs = random.randint(1, min(3, len(props))) if props else 0\n","            if num_favs > 0:\n","                favs = random.sample(props, num_favs)\n","                buyer['favourites'] = favs\n","            buyers.append(buyer)\n","\n","    # Export buyers to CSV\n","    buyers_df = pd.DataFrame(buyers)\n","    buyers_df['favourites'] = buyers_df['favourites'].apply(json.dumps)\n","    buyers_output = os.path.join(output_folder, 'buyers.csv')\n","    buyers_df.to_csv(buyers_output, index=False)\n","\n","# Main function to process CSV files and generate output files\n","def process_files(input_folder):\n","    all_properties = []  # List of all properties (flat) for the PropertyOnSale CSV\n","    sellers = {}         # Dictionary: key = agency_name, value = dict with seller info and a list of properties\n","\n","    # Process each file properties_*.csv\n","    for file_name in os.listdir(input_folder):\n","        if file_name.startswith('properties_') and file_name.endswith('.csv'):\n","            # Assuming file name format is properties_<city>_<state>.csv\n","            city, state = file_name.replace('properties_', '').replace('.csv', '').split('_')\n","            file_path = os.path.join(input_folder, file_name)\n","            columns_to_load = ['latLong', 'addressCity', 'address', 'unformattedPrice', 'imgSrc',\n","                                 'statusText', 'area', 'beds', 'baths', 'flexFieldText', 'carouselPhotos',\n","                                 'brokerName', 'hdpData', 'zestimate', 'hasOpenHouse', 'openHouseDescription']\n","            df = pd.read_csv(file_path, usecols=columns_to_load)\n","\n","            for _, row in df.iterrows():\n","                # Extract coordinates\n","                lat, lon = (None, None)\n","                if 'latLong' in row and pd.notna(row['latLong']):\n","                    try:\n","                        lat_lon_dict = ast.literal_eval(row['latLong'])\n","                        if isinstance(lat_lon_dict, dict) and 'latitude' in lat_lon_dict and 'longitude' in lat_lon_dict:\n","                            lat, lon = lat_lon_dict['latitude'], lat_lon_dict['longitude']\n","                    except (ValueError, SyntaxError):\n","                        pass\n","\n","                # Parse hdpData\n","                hdp_data = {}\n","                if 'hdpData' in row and pd.notna(row['hdpData']):\n","                    try:\n","                        hdp_data = json.loads(row['hdpData']) if isinstance(row['hdpData'], str) else row['hdpData']\n","                    except (json.JSONDecodeError, TypeError):\n","                        pass\n","\n","                # If neighbourhood is missing, calculate the nearest one\n","                neighbourhood = None\n","                if not neighbourhood and lat and lon:\n","                    neighbourhood = get_neighbourhood(lat, lon, city)\n","\n","                area_value = row.get('area', 0)\n","                max_attendees = random.randint(8, 15)\n","\n","                # Extract agency name from brokerName field\n","                temp_agency = row.get('brokerName', '')\n","                if pd.notna(temp_agency):\n","                    if 'Listing by:' in temp_agency:\n","                        agency_name = temp_agency.split('Listing by:')[1].strip()\n","                    else:\n","                        agency_name = temp_agency.strip()\n","                else:\n","                    agency_name = ''\n","\n","                # Build dynamic description\n","                description_parts = []\n","                if 'homeType' in hdp_data:\n","                    description_parts.append(f\"üè° Property type: {hdp_data['homeType']}\")\n","                if 'livingArea' in hdp_data:\n","                    description_parts.append(f\"üìè Living area: {hdp_data['livingArea']} sqft\")\n","                if 'taxAssessedValue' in hdp_data:\n","                    description_parts.append(f\"üí∏ Tax assessed value: ${hdp_data['taxAssessedValue']:,}\")\n","                if 'lotAreaValue' in hdp_data:\n","                    description_parts.append(f\"üå≥ Lot size: {hdp_data['lotAreaValue']} acres\")\n","                if pd.notna(row.get('zestimate')):\n","                    description_parts.append(f\"üí∞ Hxestimate: ${row['zestimate']:,}\")\n","                if agency_name != '':\n","                    description_parts.append(f\"üëî Agency: {agency_name}\")\n","\n","                description = \"\\n\".join(description_parts)\n","\n","\n","                # Set registration_date as current date minus days on Zillow (if available)\n","                if 'daysOnZillow' in hdp_data:\n","                    registration_date = (datetime.now() - timedelta(days=hdp_data['daysOnZillow'])).isoformat()\n","                else:\n","                    registration_date = datetime.now().isoformat()\n","\n","                # --- Begin updated open house time formatting code ---\n","                # Generate a random day of the week for open house\n","                days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n","                open_house_day = random.choice(days_of_week)\n","\n","                # Retrieve the open house time from the row data\n","                open_house_time = row.get('openHouseDescription', '')\n","\n","                # Helper function to generate a random time range (morning or afternoon)\n","                def random_time_range():\n","                    period = random.choice([\"AM\", \"PM\"])\n","                    if period == \"AM\":\n","                        random_hour = random.randint(8, 11)  # Morning hours between 8 and 11\n","                        start = f\"{random_hour}:00 {period}\"\n","                        end = f\"{random_hour + 1}:00 {period}\"\n","                    else:\n","                        # For PM, choose a random hour between 12 and 18, and convert to 12-hour format if needed\n","                        random_hour = random.randint(12, 18)\n","                        display_hour = random_hour if random_hour == 12 else random_hour - 12\n","                        start = f\"{display_hour}:00 {period}\"\n","                        # Ensure proper progression (if start is 11:00 PM, end becomes 12:00 PM, which is acceptable)\n","                        if display_hour == 11:\n","                            end = f\"12:00 {period}\"\n","                        else:\n","                            end = f\"{display_hour + 1}:00 {period}\"\n","                    return f\"{start} - {end}\"\n","\n","                if open_house_time and pd.notna(open_house_time):\n","                    # Remove any prefixes such as \"Open House:\" or \"Open House -\"\n","                    if 'Open House:' in open_house_time:\n","                        open_house_time = open_house_time.split('Open House:')[1].strip()\n","                    elif 'Open House -' in open_house_time:\n","                        open_house_time = open_house_time.split('Open House -')[1].strip()\n","\n","                    # Check if the time is provided as a range using a hyphen\n","                    if '-' in open_house_time:\n","                        parts = open_house_time.split('-', 1)\n","                        start_time = parts[0].strip()\n","                        end_time = parts[1].strip()\n","\n","                        # If the start time does not contain \"AM\" or \"PM\" but the end time does,\n","                        # append the period from end_time to start_time.\n","                        if not (\"AM\" in start_time or \"PM\" in start_time) and (\"AM\" in end_time or \"PM\" in end_time):\n","                            period = end_time.split()[-1]  # Extract the period (\"AM\" or \"PM\") from end_time\n","                            start_time = f\"{start_time} {period}\"\n","                            open_house_time = f\"{start_time} - {end_time}\"\n","                        else:\n","                            # If times are already properly formatted (either morning or afternoon), use them as is.\n","                            open_house_time = f\"{start_time} - {end_time}\"\n","                    else:\n","                        # If there is no hyphen (i.e., not a range), assign a random time range\n","                        open_house_time = random_time_range()\n","                else:\n","                    # If no open house time is provided, assign a random time range\n","                    open_house_time = random_time_range()\n","                # --- End updated open house time formatting code ---\n","\n","                property_type = row.get('statusText', '')\n","                if property_type and 'for sale' in property_type:\n","                    property_type = property_type.split('for sale')[0].strip()\n","\n","                if pd.notna(row.get('carouselPhotos')):\n","                    try:\n","                        if isinstance(row['carouselPhotos'], str):\n","                            photos = json.loads(row['carouselPhotos'].replace(\"'\", \"\\\"\"))\n","                        elif isinstance(row['carouselPhotos'], list):\n","                            photos = row['carouselPhotos']\n","                        else:\n","                            photos = []\n","\n","                        photo_urls = [photo.get('url') for photo in photos if isinstance(photo, dict) and 'url' in photo]\n","\n","                    except json.JSONDecodeError:\n","                        photo_urls = []\n","                else:\n","                    photo_urls = []\n","\n","                # Generate a unique property _id\n","                property_id = str(ObjectId())\n","\n","                # Build the flat property record (for PropertyOnSale)\n","                # Also include latitude and longitude for coordinates\n","                property_record = {\n","                    '_id': property_id,\n","                    'city': city.title(),\n","                    'neighbourhood': neighbourhood,\n","                    'address': row.get('address', ''),\n","                    'price': row.get('unformattedPrice', 0),\n","                    'thumbnail': row.get('imgSrc', ''),\n","                    'type': property_type,\n","                    'bed_number': row.get('beds', 0),\n","                    'bath_number': row.get('baths', 0),\n","                    'area': area_value,\n","                    'photos': photo_urls,\n","                    'registration_date': registration_date,\n","                    'disponibility': {\n","                        'day': open_house_day,\n","                        'time': open_house_time,\n","                        'max_attendees': max_attendees\n","                    },\n","                    # Temporary field to group by agency\n","                    'agency_name': agency_name,\n","                    'latitude': lat,\n","                    'longitude': lon\n","                }\n","\n","                if not pd.isna(description):\n","                    property_record['description'] = description\n","\n","                all_properties.append(property_record)\n","\n","                # Group properties by agency (seller)\n","                if agency_name not in sellers:\n","                    sellers[agency_name] = {\n","                        '_id': str(ObjectId()),\n","                        'agency_name': agency_name,\n","                        'email': '',      # to be generated if missing\n","                        'password': '',   # to be generated if missing\n","                        'properties': []  # temporary list of properties for this seller\n","                    }\n","                sellers[agency_name]['properties'].append(property_record)\n","\n","    # For each seller, generate email and password if missing\n","    for seller in sellers.values():\n","        if not seller['email']:\n","            if seller['agency_name']:\n","                seller['email'] = f\"{seller['agency_name'].replace(' ', '').lower()}@seller.com\"\n","            else:\n","                seller['email'] = fake.email()\n","        if not seller['password']:\n","            seller['password'] = hash_password(fake.password())\n","\n","    # Separate properties into:\n","    # - property_on_sale: with fields _id, city, neighbourhood, address, price, thumbnail and disponibility\n","    # - sold_property: randomly select some properties and add sell_date (2 days after registration_date)\n","    sellers_final = []\n","    properties_on_sale_flat = []  # List of unsold properties for the PropertyOnSale CSV\n","    for seller in sellers.values():\n","        properties_list = seller.pop('properties')\n","        sold_properties = []\n","        on_sale_properties = []\n","\n","        # Randomly decide for each property if it is sold (30% probability)\n","        for prop in properties_list:\n","            if random.random() < 0.3:\n","                reg_date = datetime.fromisoformat(prop['registration_date'])\n","                sell_date = (reg_date + timedelta(days=2)).isoformat()\n","                sold_prop = {\n","                    '_id': prop['_id'],\n","                    'city': prop['city'],\n","                    'neighbourhood': prop['neighbourhood'],\n","                    'price': prop['price'],\n","                    'thumbnail': prop['thumbnail'],\n","                    'type': prop['type'],\n","                    'area': prop['area'],\n","                    'registration_date': prop['registration_date'],\n","                    'sell_date': sell_date\n","                }\n","                sold_properties.append(sold_prop)\n","            else:\n","                on_sale_prop = {\n","                    '_id': prop['_id'],\n","                    'city': prop['city'],\n","                    'neighbourhood': prop['neighbourhood'],\n","                    'address': prop['address'],\n","                    'price': prop['price'],\n","                    'thumbnail': prop['thumbnail'],\n","                    'disponibility': prop['disponibility']\n","                }\n","                on_sale_properties.append(on_sale_prop)\n","                # Also add the property to the global list for the PropertyOnSale CSV\n","                properties_on_sale_flat.append(prop)\n","\n","        seller['sold_property'] = sold_properties\n","        seller['property_on_sale'] = on_sale_properties\n","        sellers_final.append(seller)\n","\n","    # Export sellers (ex-agencies) to CSV\n","    # Convert embedded arrays to JSON strings\n","    sellers_df = pd.DataFrame(sellers_final)\n","    sellers_df['property_on_sale'] = sellers_df['property_on_sale'].apply(json.dumps)\n","    sellers_df['sold_property'] = sellers_df['sold_property'].apply(json.dumps)\n","    sellers_output = os.path.join(output_folder, 'sellers.csv')\n","    sellers_df.to_csv(sellers_output, index=False)\n","\n","    # Export PropertyOnSale (flat) CSV\n","    # Ensure nested fields (photos and disponibility) are converted to strings\n","    for prop in properties_on_sale_flat:\n","        prop['photos'] = json.dumps(prop['photos'])\n","        prop['disponibility'] = json.dumps(prop['disponibility'])\n","    properties_df = pd.DataFrame(properties_on_sale_flat)\n","    # Remove agency_name field from properties\n","    properties_df.drop(columns=['agency_name'], inplace=True)\n","    properties_output = os.path.join(output_folder, 'properties_on_sale.csv')\n","    properties_df.to_csv(properties_output, index=False)\n","\n","    # Generate properties_on_sale_neo4j.csv file\n","    neo4j_records = []\n","    for prop in properties_on_sale_flat:\n","        lat = prop.get('latitude', '')\n","        lon = prop.get('longitude', '')\n","        try:\n","            price = float(prop.get('price', 0))\n","        except:\n","            price = 0.0\n","        score = random.uniform(0, 100)\n","        neo4j_records.append({\n","            'property_on_sale_id': prop['_id'],\n","            'latitude': lat,\n","            'longitude': lon,\n","            'price': price,\n","            'score': score,\n","            'type': prop['type'],\n","            'thumbnail': prop['thumbnail'],\n","            'neighbourhood': prop['neighbourhood']\n","        })\n","    neo4j_df = pd.DataFrame(neo4j_records)\n","    neo4j_output = os.path.join(output_folder, 'properties_on_sale_neo4j.csv')\n","    neo4j_df.to_csv(neo4j_output, index=False)\n","\n","    # Generate buyers based on unsold properties\n","    generate_buyers(properties_on_sale_flat, output_folder)\n","\n","# Input/output paths and loading the neighbourhoods file\n","input_folder = '/drive/MyDrive/Intermediate Corporation/Dati/Zillow/Scraped'\n","output_folder = '/drive/MyDrive/Intermediate Corporation/Dati/Zillow/Formatted'\n","neighbourhoods_df = pd.read_csv('/drive/MyDrive/Intermediate Corporation/Dati/Cities and neighbourhoods/neighbourhoods.csv')\n","neighbourhoods_df = neighbourhoods_df[neighbourhoods_df[\"name\"] != \"Unknown\"]\n","\n","print(os.listdir(input_folder))\n","print(os.listdir(output_folder))\n","process_files(input_folder)\n"]}]}